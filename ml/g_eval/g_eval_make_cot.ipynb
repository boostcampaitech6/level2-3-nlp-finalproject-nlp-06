{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-Eval\n",
    "\n",
    "* Naturalness(1-5) : 응답이 사람의 자연스러운 발화와 같은지 판단\n",
    "* coherence (1-5) : 이전 대화와 이어지는지 (연속적인지) 판단\n",
    "* Engagingness (1-3) : 응답이 흥미로운지 지루한지 판단\n",
    "* Groundedness (1-3) : 응답이 사실을 포함하는지\n",
    "\n",
    "----\n",
    "\n",
    "논문 원문에서는 Engagingness 지표에 대한 scale & definition만 나와있으므로 나머지는 직접 설계해야 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT api call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv_file = dotenv.find_dotenv()\n",
    "dotenv.load_dotenv(dotenv_file)\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT 생성 (GPT-4)\n",
    "\n",
    "* input : 평가할 대상 작업의 정의, 원하는 평가 기준\n",
    "* engagingness는 예시로 score 평가 방법까지 주어짐. 나머지는 crietreria에 대한 정의만 주자.\n",
    "> criteria에 대한 정의는 선행 논문 (UniEval) 에서 주어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagingness_prompt = f\"\"\"You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well.\n",
    "Your task is to rate the responses on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Engagingness (1-3) Is the response dull/interesting?\n",
    "Determine if the response is interesting or dull.\n",
    "- A score of 1 (dull) means that the response is generic and dull.\n",
    "- A score of 2 (somewhat interesting) means the response is somewhat interesting and could\n",
    "engage you in the conversation (e.g., an opinion, thought)\n",
    "- A score of 3 (interesting) means the response is very interesting or presents an interesting fact\n",
    "\n",
    "Evaluation Steps:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naturalness_prompt = f\"\"\"You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well.\n",
    "Your task is to rate the responses on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Naturalness (1-5) Is this a natural response in the dialogue?\n",
    "Judge whether a response is like something a person would naturally say.\n",
    "\n",
    "Evaluation Steps:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_prompt = f\"\"\"You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well.\n",
    "Your task is to rate the responses on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence (1-5) Is this a coherent response given the dialogue history?\n",
    "Determine whether this response serves as a valid continuation of the previous conversation.\n",
    "\n",
    "Evaluation Steps:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_prompt = f\"\"\"You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well.\n",
    "Your task is to rate the responses on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Groundedness (1-3) Does this response use knowledge from the fact?\n",
    "Given the fact that this response is conditioned on, determine whether this response uses that fact.\n",
    "\n",
    "- A score of 1 means the response is not based on facts.\n",
    "- A score of 2 means that the response is somewhat factual, but incorrect.\n",
    "- A score of 3 means the response is well-written and based on facts.\n",
    "\n",
    "Evaluation Steps:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [engagingness_prompt, naturalness_prompt, coherence_prompt, groundedness_prompt]\n",
    "cot_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                            model=\"gpt-4\",\n",
    "                            messages=[{\"role\": \"system\", \"content\": p}],\n",
    "                            # temperature=2,\n",
    "                            # max_tokens=5,\n",
    "                            # top_p=1,\n",
    "                            # frequency_penalty=0,\n",
    "                            # presence_penalty=0,\n",
    "                            # stop=None,\n",
    "                            # # logprobs=40,\n",
    "                            # n=20\n",
    "    )\n",
    "    print(p)\n",
    "    print(response.choices[0].message.content)\n",
    "    cot_list.append(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, p in enumerate(prompts):\n",
    "    prompts[idx] += cot_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"engagingness.txt\", \"wt\").write(prompts[0])\n",
    "open(\"naturalness.txt\", \"wt\").write(prompts[1])\n",
    "open(\"coherence.txt\", \"wt\").write(prompts[2])\n",
    "open(\"groundedness.txt\", \"wt\").write(prompts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT in Korean (GPT-4)\n",
    "\n",
    "- 프롬프트 번역은 deepL로 진행 + 수동 검수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagingness_prompt = f\"\"\"두 사람 사이의 대화와 다음 턴에 나타날 수 있는 한 가지 잠재적 응답이 주어집니다. 해당 응답은 주어진 페르소나 문장을 바탕으로 생성된 것으로, 이 페르소나 문장도 함께 제공됩니다. 여러분의 임무는 하나의 지표에 대해 응답을 평가하는 것입니다. 이 지침을 주의 깊게 읽고 이해하시기 바랍니다. 검토하는 동안 이 문서를 열어 두었다가 필요할 때 참조하시기 바랍니다.\n",
    "\n",
    "평가 기준:\n",
    "참여도(1-5) 응답이 지루하거나 흥미롭지 않나요? 주어진 응답이 흥미로운지 지루한지 판단합니다.\n",
    "- 1점은 응답이 일반적이고 지루하다는 것을 의미합니다.\n",
    "- 3점은 응답이 다소 흥미롭고 대화에 참여할 수 있는 내용(예: 의견, 생각)을 담고 있음을 의미합니다.\n",
    "- 5점은 응답이 매우 흥미롭거나 흥미로운 사실을 제시한다는 의미입니다.\n",
    "\n",
    "세부 평가 단계:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naturalness_prompt = f\"\"\"두 사람 사이의 대화와 다음 턴에 나타날 수 있는 한 가지 잠재적 응답이 주어집니다. 해당 응답은 주어진 페르소나 문장을 바탕으로 생성된 것으로, 이 페르소나 문장도 함께 제공됩니다. 여러분의 임무는 하나의 지표에 대해 응답을 평가하는 것입니다. 이 지침을 주의 깊게 읽고 이해하시기 바랍니다. 검토하는 동안 이 문서를 열어 두었다가 필요할 때 참조하시기 바랍니다.\n",
    "\n",
    "평가 기준:\n",
    "자연스러움(1-5) 대화에서 자연스러운 반응인가요? 주어진 응답이 사람이 자연스럽게 할 수 있는 말과 같은지 판단합니다.\n",
    "- 1점은 응답이 매우 부자연스럽고, 사람의 말과 상당한 거리가 있는 것을 의미합니다.\n",
    "- 3점은 응답이 어느 정도 자연스러우며, 사람의 말과 일정 부분 유사함을 의미합니다.\n",
    "- 5점은 응답이 매우 자연스럽고, 사람의 말과 거의 흡사할 정도로 자연스럽다는 것을 의미합니다.\n",
    "\n",
    "세부 평가 단계:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_prompt = f\"\"\"두 사람 사이의 대화와 다음 턴에 나타날 수 있는 한 가지 잠재적 응답이 주어집니다. 해당 응답은 주어진 페르소나 문장을 바탕으로 생성된 것으로, 이 페르소나 문장도 함께 제공됩니다. 여러분의 임무는 하나의 지표에 대해 응답을 평가하는 것입니다. 이 지침을 주의 깊게 읽고 이해하시기 바랍니다. 검토하는 동안 이 문서를 열어 두었다가 필요할 때 참조하시기 바랍니다.\n",
    "\n",
    "평가 기준:\n",
    "일관성(1-5) 대화 기록을 고려할 때 일관성 있는 응답인가요? 주어진 응답이 이전 대화의 유효한 연속인지 여부를 판단합니다.\n",
    "- 1점은 응답이 매우 일관성이 없음을 의미하며 대화의 어조나 주제와 일치하지 않습니다. \n",
    "- 3점은 응답이 어느 정도 일관성이 있음을 의미하며, 주제에서 벗어나거나 어조에 완벽하게 맞지 않을 수 있지만 전체 대화에 어느 정도 부합합니다. \n",
    "- 5점은 응답이 매우 일관성이 있음을 의미하며, 대화의 주제와 어조를 모두 완벽하게 따릅니다.\n",
    "\n",
    "세부 평가 단계:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_prompt = f\"\"\"두 사람 사이의 대화와 다음 턴에 나타날 수 있는 한 가지 잠재적 응답이 주어집니다. 해당 응답은 주어진 페르소나 문장을 바탕으로 생성된 것으로, 이 페르소나 문장도 함께 제공됩니다. 여러분의 임무는 하나의 지표에 대해 응답을 평가하는 것입니다. 이 지침을 주의 깊게 읽고 이해하시기 바랍니다. 검토하는 동안 이 문서를 열어 두었다가 필요할 때 참조하시기 바랍니다.\n",
    "\n",
    "평가 기준:\n",
    "근거성(1-5) 페르소나를 고려한 응답인가요? 페르소나 문장이 주어졌을 때, 이 응답이 해당 페르소나에 기반하여 생성된 문장인지 판단합니다.\n",
    "- 1점은 답변이 사실에 근거하지 않았음을 의미합니다.\n",
    "- 3점은 답변이 어느 정도 사실에 근거하지만 부정확하다는 것을 의미합니다.\n",
    "- 5점은 답변이 사실에 근거하여 잘 작성되었음을 의미합니다.\n",
    "\n",
    "세부 평가 단계:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [engagingness_prompt, naturalness_prompt, coherence_prompt, groundedness_prompt]\n",
    "cot_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                            model=\"gpt-4\",\n",
    "                            messages=[{\"role\": \"system\", \"content\": p}],\n",
    "                            # temperature=2,\n",
    "                            # max_tokens=5,\n",
    "                            # top_p=1,\n",
    "                            # frequency_penalty=0,\n",
    "                            # presence_penalty=0,\n",
    "                            # stop=None,\n",
    "                            # # logprobs=40,\n",
    "                            # n=20\n",
    "    )\n",
    "    print(p)\n",
    "    print(response.choices[0].message.content)\n",
    "    cot_list.append(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, p in enumerate(prompts):\n",
    "    prompts[idx] += cot_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"cot/engagingness_KOR2.txt\", \"wt\").write(prompts[0])\n",
    "open(\"cot/naturalness_KOR2.txt\", \"wt\").write(prompts[1])\n",
    "open(\"cot/coherence_KOR2.txt\", \"wt\").write(prompts[2])\n",
    "open(\"cot/groundedness_KOR2.txt\", \"wt\").write(prompts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KOR2 버전에서 수동 검수를 마침 -> KOR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_prompt = open('cot/coherence_KOR3.txt').read()\n",
    "engagingness_prompt = open('cot/engagingness_KOR3.txt').read()\n",
    "naturalness_prompt = open('cot/naturalness_KOR3.txt').read()\n",
    "groundedness_prompt = open('cot/groundedness_KOR3.txt').read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "true_friend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
