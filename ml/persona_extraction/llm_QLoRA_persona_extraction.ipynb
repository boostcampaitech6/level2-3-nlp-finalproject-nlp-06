{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 26 17:33:24 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   36C    P0    51W / 300W |  10576MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     31788      C   ...onda3/envs/llm/bin/python     2452MiB |\n",
      "|    0   N/A  N/A    414733      C   ...nda3/envs/test/bin/python     8122MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\" # 'beomi/OPEN-SOLAR-KO-10.7B' \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRA setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 729403392 || trainable%: 0.21563705588032142\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# kbit training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# lora config\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"], #[\"q_proj\",\"up_proj\",\"o_proj\",\"k_proj\",\"down_proj\",\"gate_proj\",\"v_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# peft model\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 마련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>session_dialog</th>\n",
       "      <th>session_persona</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K2-10540-CL12939-CP35150-09-05-S2.json</td>\n",
       "      <td>['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이...</td>\n",
       "      <td>['나는 가끔 외로워서 반려동물을 키워보고 싶다. 나는 유기견 입양에 대해 고민하고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K2-34114-CL01643-CP00208-06-18-S2.json</td>\n",
       "      <td>['안녕. 20대 여자야. 반가워.', '아니, 나는 무교야. 너는?', '어떤 일...</td>\n",
       "      <td>['나는 종교가 없다.', '나는 펀드매니저이다.', '나는 좋아하는 음식으로 기분...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K2-33618-CL20065-CP22132-16-13-S2.json</td>\n",
       "      <td>['며칠이 지났는지 날짜도 모르겠네. 일주일이 지났어?', '시간이 너무 빨리 가....</td>\n",
       "      <td>['나는 선후배와 잘 어울리지 않는다.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K2-19873-CL20525-CP22570-06-09-S2.json</td>\n",
       "      <td>['응, 선생님은 준비 잘 하고 있어?', '선생님은 왜 가게 되셨는지 여쭤봐도 될...</td>\n",
       "      <td>['나는 영문학과이다.', '나는 음식을 배우기 위해 미국으로 간다.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K2-05407-CL01467-CP01154-14-03-S2.json</td>\n",
       "      <td>['응, 하긴 했어. 청소하는 게 짜증나. 너는 무슨 영화를 봤어?', '아니, 나...</td>\n",
       "      <td>['나는 청소하는 게 짜증 난다.', '나는 영화를 안 좋아한다.', '나는 위인전...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>K2-26129-CL23839-CP24690-03-05-S2.json</td>\n",
       "      <td>['안녕, 나는 20대 여자야. 반가워.', '뭐하고 있었어? 난 SF 영화를 가끔...</td>\n",
       "      <td>['나는 SF 영화를 가끔 본다. 나는 오늘 영화 그래비티를 재미있게 봤다.', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>K2-34387-CL21846-CP22413-03-04-S2.json</td>\n",
       "      <td>['안녕, 어제는 잘 잤어? 이제 곧 5시네.', '나도 지금 준비중이긴 한데, 지...</td>\n",
       "      <td>['나는 요즘 전기차가 많이 늘어났다고 생각한다.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>K2-06850-CL00979-CP04444-19-04-S2.json</td>\n",
       "      <td>['안녕, 나는 20대 남자야.', '오, 나이가 좀 있으신 분이시네. 어디 사세요...</td>\n",
       "      <td>['나의 고향은 전라도이다. 나는 현재 서울에 거주하고 있다.', '나는 연예인 지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>K2-07302-CL20306-CP22594-16-01-S2.json</td>\n",
       "      <td>['5시간이나 지났어? 시간 가는줄도 몰랐네, 다녀왔어.', '아니, 사실 내 입맛...</td>\n",
       "      <td>['나는 커피가 나의 입맛에 맞지 않았다.', '나는 피부가 별로이다.', '나의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>K5-03937-CL74462-CP20739-19-01-S2.json</td>\n",
       "      <td>['안녕, 20대 남자야. 또래네.', '나는 체험 여행을 가끔 가는 편이라 어딜 ...</td>\n",
       "      <td>['나는 체험 여행을 가끔 간다.', '나는 단양 패러 글라이딩 체험에 흥미가 있다...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2330 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id  \\\n",
       "0     K2-10540-CL12939-CP35150-09-05-S2.json   \n",
       "1     K2-34114-CL01643-CP00208-06-18-S2.json   \n",
       "2     K2-33618-CL20065-CP22132-16-13-S2.json   \n",
       "3     K2-19873-CL20525-CP22570-06-09-S2.json   \n",
       "4     K2-05407-CL01467-CP01154-14-03-S2.json   \n",
       "...                                      ...   \n",
       "2325  K2-26129-CL23839-CP24690-03-05-S2.json   \n",
       "2326  K2-34387-CL21846-CP22413-03-04-S2.json   \n",
       "2327  K2-06850-CL00979-CP04444-19-04-S2.json   \n",
       "2328  K2-07302-CL20306-CP22594-16-01-S2.json   \n",
       "2329  K5-03937-CL74462-CP20739-19-01-S2.json   \n",
       "\n",
       "                                         session_dialog  \\\n",
       "0     ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이...   \n",
       "1     ['안녕. 20대 여자야. 반가워.', '아니, 나는 무교야. 너는?', '어떤 일...   \n",
       "2     ['며칠이 지났는지 날짜도 모르겠네. 일주일이 지났어?', '시간이 너무 빨리 가....   \n",
       "3     ['응, 선생님은 준비 잘 하고 있어?', '선생님은 왜 가게 되셨는지 여쭤봐도 될...   \n",
       "4     ['응, 하긴 했어. 청소하는 게 짜증나. 너는 무슨 영화를 봤어?', '아니, 나...   \n",
       "...                                                 ...   \n",
       "2325  ['안녕, 나는 20대 여자야. 반가워.', '뭐하고 있었어? 난 SF 영화를 가끔...   \n",
       "2326  ['안녕, 어제는 잘 잤어? 이제 곧 5시네.', '나도 지금 준비중이긴 한데, 지...   \n",
       "2327  ['안녕, 나는 20대 남자야.', '오, 나이가 좀 있으신 분이시네. 어디 사세요...   \n",
       "2328  ['5시간이나 지났어? 시간 가는줄도 몰랐네, 다녀왔어.', '아니, 사실 내 입맛...   \n",
       "2329  ['안녕, 20대 남자야. 또래네.', '나는 체험 여행을 가끔 가는 편이라 어딜 ...   \n",
       "\n",
       "                                        session_persona  \n",
       "0     ['나는 가끔 외로워서 반려동물을 키워보고 싶다. 나는 유기견 입양에 대해 고민하고...  \n",
       "1     ['나는 종교가 없다.', '나는 펀드매니저이다.', '나는 좋아하는 음식으로 기분...  \n",
       "2                               ['나는 선후배와 잘 어울리지 않는다.']  \n",
       "3              ['나는 영문학과이다.', '나는 음식을 배우기 위해 미국으로 간다.']  \n",
       "4     ['나는 청소하는 게 짜증 난다.', '나는 영화를 안 좋아한다.', '나는 위인전...  \n",
       "...                                                 ...  \n",
       "2325  ['나는 SF 영화를 가끔 본다. 나는 오늘 영화 그래비티를 재미있게 봤다.', '...  \n",
       "2326                      ['나는 요즘 전기차가 많이 늘어났다고 생각한다.']  \n",
       "2327  ['나의 고향은 전라도이다. 나는 현재 서울에 거주하고 있다.', '나는 연예인 지...  \n",
       "2328  ['나는 커피가 나의 입맛에 맞지 않았다.', '나는 피부가 별로이다.', '나의 ...  \n",
       "2329  ['나는 체험 여행을 가끔 간다.', '나는 단양 패러 글라이딩 체험에 흥미가 있다...  \n",
       "\n",
       "[2330 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_df = pd.read_csv('filtered_total.csv')\n",
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [no persona] special token 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[no_persona] 개수 :  125\n",
      "------------------------------------------------------------------------------------------------\n",
      "default tokenizer vocab_size : 30003\n",
      "default tokenizer의 [no_persona] tokenizing : [62, 81, 82, 66, 83, 13151, 2600, 68, 64]\n",
      "------------------------------------------------------------------------------------------------\n",
      "added tokenizer vocab_size : 30004\n",
      "added tokenizer의 [no_persona] tokenizing : [30003]\n",
      "------------------------------------------------------------------------------------------------\n",
      "added tokenizer의 special_tokens : {'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[no_persona]']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30004, 2048)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[no_persona] 개수 : \", len(total_df[total_df['session_persona']==\"['[no_persona]']\"]))\n",
    "print('------------------------------------------------------------------------------------------------')\n",
    "print('default tokenizer vocab_size :', len(tokenizer))\n",
    "print(\"default tokenizer의 [no_persona] tokenizing :\", tokenizer.encode(\"[no_persona]\"))\n",
    "\n",
    "# tokenizer에 [no_persona] 추가\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"[no_persona]\"]})\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------')\n",
    "print('added tokenizer vocab_size :', len(tokenizer))\n",
    "print(\"added tokenizer의 [no_persona] tokenizing :\", tokenizer.encode(\"[no_persona]\"))\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------')\n",
    "print('added tokenizer의 special_tokens :', tokenizer.special_tokens_map)\n",
    "\n",
    "# model resize\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedata(ses_dialog, ses_persona):\n",
    "        return f\"### 사용자 채팅: {ses_dialog}</끝>\\n### 사용자 페르소나: {ses_persona}</끝>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2330it [00:00, 4659.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_df['text']=None\n",
    "\n",
    "for i,row in tqdm(total_df.iterrows()):\n",
    "    total_df.loc[i,'text']=(makedata(row['session_dialog'], row['session_persona']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>session_dialog</th>\n",
       "      <th>session_persona</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K2-10540-CL12939-CP35150-09-05-S2.json</td>\n",
       "      <td>['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이...</td>\n",
       "      <td>['나는 가끔 외로워서 반려동물을 키워보고 싶다. 나는 유기견 입양에 대해 고민하고...</td>\n",
       "      <td>### 사용자 채팅: ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K2-34114-CL01643-CP00208-06-18-S2.json</td>\n",
       "      <td>['안녕. 20대 여자야. 반가워.', '아니, 나는 무교야. 너는?', '어떤 일...</td>\n",
       "      <td>['나는 종교가 없다.', '나는 펀드매니저이다.', '나는 좋아하는 음식으로 기분...</td>\n",
       "      <td>### 사용자 채팅: ['안녕. 20대 여자야. 반가워.', '아니, 나는 무교야....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K2-33618-CL20065-CP22132-16-13-S2.json</td>\n",
       "      <td>['며칠이 지났는지 날짜도 모르겠네. 일주일이 지났어?', '시간이 너무 빨리 가....</td>\n",
       "      <td>['나는 선후배와 잘 어울리지 않는다.']</td>\n",
       "      <td>### 사용자 채팅: ['며칠이 지났는지 날짜도 모르겠네. 일주일이 지났어?', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K2-19873-CL20525-CP22570-06-09-S2.json</td>\n",
       "      <td>['응, 선생님은 준비 잘 하고 있어?', '선생님은 왜 가게 되셨는지 여쭤봐도 될...</td>\n",
       "      <td>['나는 영문학과이다.', '나는 음식을 배우기 위해 미국으로 간다.']</td>\n",
       "      <td>### 사용자 채팅: ['응, 선생님은 준비 잘 하고 있어?', '선생님은 왜 가게...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K2-05407-CL01467-CP01154-14-03-S2.json</td>\n",
       "      <td>['응, 하긴 했어. 청소하는 게 짜증나. 너는 무슨 영화를 봤어?', '아니, 나...</td>\n",
       "      <td>['나는 청소하는 게 짜증 난다.', '나는 영화를 안 좋아한다.', '나는 위인전...</td>\n",
       "      <td>### 사용자 채팅: ['응, 하긴 했어. 청소하는 게 짜증나. 너는 무슨 영화를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>K2-26129-CL23839-CP24690-03-05-S2.json</td>\n",
       "      <td>['안녕, 나는 20대 여자야. 반가워.', '뭐하고 있었어? 난 SF 영화를 가끔...</td>\n",
       "      <td>['나는 SF 영화를 가끔 본다. 나는 오늘 영화 그래비티를 재미있게 봤다.', '...</td>\n",
       "      <td>### 사용자 채팅: ['안녕, 나는 20대 여자야. 반가워.', '뭐하고 있었어?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>K2-34387-CL21846-CP22413-03-04-S2.json</td>\n",
       "      <td>['안녕, 어제는 잘 잤어? 이제 곧 5시네.', '나도 지금 준비중이긴 한데, 지...</td>\n",
       "      <td>['나는 요즘 전기차가 많이 늘어났다고 생각한다.']</td>\n",
       "      <td>### 사용자 채팅: ['안녕, 어제는 잘 잤어? 이제 곧 5시네.', '나도 지금...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>K2-06850-CL00979-CP04444-19-04-S2.json</td>\n",
       "      <td>['안녕, 나는 20대 남자야.', '오, 나이가 좀 있으신 분이시네. 어디 사세요...</td>\n",
       "      <td>['나의 고향은 전라도이다. 나는 현재 서울에 거주하고 있다.', '나는 연예인 지...</td>\n",
       "      <td>### 사용자 채팅: ['안녕, 나는 20대 남자야.', '오, 나이가 좀 있으신 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>K2-07302-CL20306-CP22594-16-01-S2.json</td>\n",
       "      <td>['5시간이나 지났어? 시간 가는줄도 몰랐네, 다녀왔어.', '아니, 사실 내 입맛...</td>\n",
       "      <td>['나는 커피가 나의 입맛에 맞지 않았다.', '나는 피부가 별로이다.', '나의 ...</td>\n",
       "      <td>### 사용자 채팅: ['5시간이나 지났어? 시간 가는줄도 몰랐네, 다녀왔어.', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>K5-03937-CL74462-CP20739-19-01-S2.json</td>\n",
       "      <td>['안녕, 20대 남자야. 또래네.', '나는 체험 여행을 가끔 가는 편이라 어딜 ...</td>\n",
       "      <td>['나는 체험 여행을 가끔 간다.', '나는 단양 패러 글라이딩 체험에 흥미가 있다...</td>\n",
       "      <td>### 사용자 채팅: ['안녕, 20대 남자야. 또래네.', '나는 체험 여행을 가...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2330 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id  \\\n",
       "0     K2-10540-CL12939-CP35150-09-05-S2.json   \n",
       "1     K2-34114-CL01643-CP00208-06-18-S2.json   \n",
       "2     K2-33618-CL20065-CP22132-16-13-S2.json   \n",
       "3     K2-19873-CL20525-CP22570-06-09-S2.json   \n",
       "4     K2-05407-CL01467-CP01154-14-03-S2.json   \n",
       "...                                      ...   \n",
       "2325  K2-26129-CL23839-CP24690-03-05-S2.json   \n",
       "2326  K2-34387-CL21846-CP22413-03-04-S2.json   \n",
       "2327  K2-06850-CL00979-CP04444-19-04-S2.json   \n",
       "2328  K2-07302-CL20306-CP22594-16-01-S2.json   \n",
       "2329  K5-03937-CL74462-CP20739-19-01-S2.json   \n",
       "\n",
       "                                         session_dialog  \\\n",
       "0     ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이...   \n",
       "1     ['안녕. 20대 여자야. 반가워.', '아니, 나는 무교야. 너는?', '어떤 일...   \n",
       "2     ['며칠이 지났는지 날짜도 모르겠네. 일주일이 지났어?', '시간이 너무 빨리 가....   \n",
       "3     ['응, 선생님은 준비 잘 하고 있어?', '선생님은 왜 가게 되셨는지 여쭤봐도 될...   \n",
       "4     ['응, 하긴 했어. 청소하는 게 짜증나. 너는 무슨 영화를 봤어?', '아니, 나...   \n",
       "...                                                 ...   \n",
       "2325  ['안녕, 나는 20대 여자야. 반가워.', '뭐하고 있었어? 난 SF 영화를 가끔...   \n",
       "2326  ['안녕, 어제는 잘 잤어? 이제 곧 5시네.', '나도 지금 준비중이긴 한데, 지...   \n",
       "2327  ['안녕, 나는 20대 남자야.', '오, 나이가 좀 있으신 분이시네. 어디 사세요...   \n",
       "2328  ['5시간이나 지났어? 시간 가는줄도 몰랐네, 다녀왔어.', '아니, 사실 내 입맛...   \n",
       "2329  ['안녕, 20대 남자야. 또래네.', '나는 체험 여행을 가끔 가는 편이라 어딜 ...   \n",
       "\n",
       "                                        session_persona  \\\n",
       "0     ['나는 가끔 외로워서 반려동물을 키워보고 싶다. 나는 유기견 입양에 대해 고민하고...   \n",
       "1     ['나는 종교가 없다.', '나는 펀드매니저이다.', '나는 좋아하는 음식으로 기분...   \n",
       "2                               ['나는 선후배와 잘 어울리지 않는다.']   \n",
       "3              ['나는 영문학과이다.', '나는 음식을 배우기 위해 미국으로 간다.']   \n",
       "4     ['나는 청소하는 게 짜증 난다.', '나는 영화를 안 좋아한다.', '나는 위인전...   \n",
       "...                                                 ...   \n",
       "2325  ['나는 SF 영화를 가끔 본다. 나는 오늘 영화 그래비티를 재미있게 봤다.', '...   \n",
       "2326                      ['나는 요즘 전기차가 많이 늘어났다고 생각한다.']   \n",
       "2327  ['나의 고향은 전라도이다. 나는 현재 서울에 거주하고 있다.', '나는 연예인 지...   \n",
       "2328  ['나는 커피가 나의 입맛에 맞지 않았다.', '나는 피부가 별로이다.', '나의 ...   \n",
       "2329  ['나는 체험 여행을 가끔 간다.', '나는 단양 패러 글라이딩 체험에 흥미가 있다...   \n",
       "\n",
       "                                                   text  \n",
       "0     ### 사용자 채팅: ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을...  \n",
       "1     ### 사용자 채팅: ['안녕. 20대 여자야. 반가워.', '아니, 나는 무교야....  \n",
       "2     ### 사용자 채팅: ['며칠이 지났는지 날짜도 모르겠네. 일주일이 지났어?', '...  \n",
       "3     ### 사용자 채팅: ['응, 선생님은 준비 잘 하고 있어?', '선생님은 왜 가게...  \n",
       "4     ### 사용자 채팅: ['응, 하긴 했어. 청소하는 게 짜증나. 너는 무슨 영화를 ...  \n",
       "...                                                 ...  \n",
       "2325  ### 사용자 채팅: ['안녕, 나는 20대 여자야. 반가워.', '뭐하고 있었어?...  \n",
       "2326  ### 사용자 채팅: ['안녕, 어제는 잘 잤어? 이제 곧 5시네.', '나도 지금...  \n",
       "2327  ### 사용자 채팅: ['안녕, 나는 20대 남자야.', '오, 나이가 좀 있으신 ...  \n",
       "2328  ### 사용자 채팅: ['5시간이나 지났어? 시간 가는줄도 몰랐네, 다녀왔어.', ...  \n",
       "2329  ### 사용자 채팅: ['안녕, 20대 남자야. 또래네.', '나는 체험 여행을 가...  \n",
       "\n",
       "[2330 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### 사용자 채팅: ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이 많더라.', '대단하네. 나도 가끔 외로워서 반려동물을 키워보고 싶은데, 유기견을 입양해볼까 고민이 많아.', '맞아. 유기견 봉사활동 같은 것도 많은 것 같아. 나중에 기회가 되면 한번 가봐야겠어.', '요즘 밴드나 알아볼 수 있는 방법들이 많다고 들었어.  찾아보면 될 것 같아.', '맞아, 혼자 가서 할 수 있는 일도 아니야. 한번 알아봐.', '아니, 나도 관심은 있는데, 키우기는 조금 부담이 돼.']</끝>\\n### 사용자 페르소나: ['나는 가끔 외로워서 반려동물을 키워보고 싶다. 나는 유기견 입양에 대해 고민하고 있다.', '나는 반려동물에 관심이 있다. 나는 반려동물을 키우기에 부담이 된다.']</끝>\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2330.000000\n",
       "mean      221.142918\n",
       "std        51.802716\n",
       "min       114.000000\n",
       "25%       185.000000\n",
       "50%       214.000000\n",
       "75%       248.000000\n",
       "max       569.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 길이 통계\n",
    "total_df['text'].apply(lambda x: len(tokenizer.encode(x))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2330/2330 [00:01<00:00, 1389.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing 후, Dataset 형태로 준비\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encodings[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "\n",
    "tokenized = []\n",
    "for text in tqdm(total_df['text']):\n",
    "    encoded = tokenizer.encode(text,return_tensors='pt',\n",
    "                               return_token_type_ids=False, \n",
    "                               padding='max_length', \n",
    "                               max_length=600, # max_length 고려\n",
    "                               truncation=True, \n",
    "                               return_attention_mask=False)\n",
    "    tokenized.append(encoded)\n",
    "    \n",
    "dataset = Dataset(torch.cat(tokenized,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    6,     6,     6, 12238, 16262,    29,  5485,    10, 13417,    15,\n",
       "         1010,  7499,   348,    34,   401,   393,   380,   270,   978,    17,\n",
       "         3042,   514,  1193,    15,  4851,  7030,  5447,   276,  4340,    34,\n",
       "         3090,  5167,  1055,   270,   750,  4325,    17,  3042,   514,   360,\n",
       "          539,   284,   978,    17,   462,   309,  6609,   882,   286,  6878,\n",
       "         7030,  5447,   276,  4340,  6571,  1219,  3200,    15,  5167,  1055,\n",
       "          276, 12025,   310, 15259,  2387,   270,   750,   399,    17,  3042,\n",
       "          514,  3108,   399,    17,  5167,  1055,  3715,  2032,   734,   296,\n",
       "          388,   309,   750,   296,   388,   734,   399,    17,  4788,   274,\n",
       "         1903,   293,   740,   378,  5876,   409,  4581,   685,   348,    17,\n",
       "         3042,   514, 10162,  7708,   392, 17715,   365,   327,   272,  1891,\n",
       "         6214,   750,   682,   705,   450,   348,    17,   224,  8454,   378,\n",
       "         1180,   388,   734,   399,    17,  3042,   514,  3108,   399,    15,\n",
       "         3193,   409,   306,   823,   365,   327,   272,   477,   309, 23633,\n",
       "           17,  5876,  3494,  2677,    17,  3042,   514,  4765,    15,   462,\n",
       "          309,  1479,   296,   327,   829,    15,  4009,   316,   272,  1922,\n",
       "         1863,   270,  1627,    17,    10,    64,    31,    18,  5568,    33,\n",
       "          202,     6,     6,     6, 12238, 13001,   483,   392,    29,  5485,\n",
       "           10,   392,   272,  6609,   882,   286,  6878,  7030,  5447,   276,\n",
       "         4340,  6571,  1219,   267,    17,   462,   272,  5167,  1055, 12025,\n",
       "          274,   879,  2387,  4296,   327,   267,    17,  3042,   514,   392,\n",
       "          272,  7030,  5447,   274,  1479,   270,   327,   267,    17,   462,\n",
       "          272,  7030,  5447,   276,  4009,  4287,  1863,   270,  1265,    17,\n",
       "           10,    64,    31,    18,  5568,    33,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trian Valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9대 1로 train,dev 분할\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, dev_dataset = train_test_split(dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2097, 233)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "## Trainer\n",
    "\n",
    "early_callback=transformers.EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    # eval_dataset=dev_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./outputs\",\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=True,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=30,\n",
    "        # report_to=[\"tensorboard\"],\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate=1e-4,\n",
    "        # resume_from_checkpoint=True,\n",
    "        lr_scheduler_type= \"cosine\", # \"constant\"\n",
    "        # load_best_model_at_end = True,\n",
    "        # evaluation_strategy='epoch',\n",
    "        save_strategy='steps',\n",
    "        #optim=\"paged_adamw_8bit\"\n",
    "        # vload_best_model_at_end=True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    # callbacks=[early_callback],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "## SFTTrainer\n",
    "\n",
    "from trl import SFTTrainer\n",
    "# SFTTrainer 사용할 경우\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./outputs\",\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=True,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=30,\n",
    "        # report_to=[\"tensorboard\"],\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate=1e-4,\n",
    "        # resume_from_checkpoint=True,\n",
    "        lr_scheduler_type= \"cosine\", # \"constant\"\n",
    "        # load_best_model_at_end = True,\n",
    "        # evaluation_strategy='epoch',\n",
    "        # save_strategy='steps',\n",
    "        # optim=\"paged_adamw_8bit\"\n",
    "        # load_best_model_at_end=True\n",
    "    ),\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [292/292 36:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>6.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>6.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>6.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>6.696900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=292, training_loss=6.714487101933727, metrics={'train_runtime': 2176.832, 'train_samples_per_second': 1.07, 'train_steps_per_second': 0.134, 'total_flos': 8.9681796808704e+16, 'train_loss': 6.714487101933727, 'epoch': 1.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved/beomi-OPEN-SOLAR-KO-10.7B-1ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "stop_words = [\"</끝>\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze().to('cuda') for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(chat=\"\"):\n",
    "    prompt = f\"### 사용자 채팅: {chat}</끝>\\n### 사용자 페르소나: \"\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "    tokenized = {k: v.to('cuda') for k, v in tokenized.items()}\n",
    "    gened = model.generate(\n",
    "        **tokenized,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.6,\n",
    "        num_beams=3,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    return tokenizer.decode(gened[0]).replace(prompt+\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(chat=\"\"):\n",
    "    prompt = f\"### 사용자 채팅: {chat}</끝>\\n### 사용자 페르소나: \"\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False,\n",
    "        ),\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.6,\n",
    "        num_beams=3,\n",
    "        # stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    return tokenizer.decode(gened[0]).replace(prompt+\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ['안녕, 잘 지냈어? 2일만이네.', \n",
    "        '응, 혹시 반려동물을 키워? 요즘 유기견이 많더라.', \n",
    "        '대단하네. 나도 가끔 외로워서 반려동물을 키워보고 싶은데, 유기견을 입양해볼까 고민이 많아.', \n",
    "        '맞아. 유기견 봉사활동 같은 것도 많은 것 같아. 나중에 기회가 되면 한번 가봐야겠어.', \n",
    "        '요즘 밴드나 알아볼 수 있는 방법들이 많다고 들었어.  찾아보면 될 것 같아.', \n",
    "        '맞아, 혼자 가서 할 수 있는 일도 아니야. 한번 알아봐.', \n",
    "        '아니, 나도 관심은 있는데, 키우기는 조금 부담이 돼.']\n",
    "\n",
    "target = ['나는 가끔 외로워서 반려동물을 키워보고 싶다. 나는 유기견 입양에 대해 고민하고 있다.', \n",
    "          '나는 반려동물에 관심이 있다. 나는 반려동물을 키우기에 부담이 된다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### 사용자 채팅: ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이 많더라.', '대단하네. 나도 가끔 외로워서 반려동물을 키워보고 싶은데, 유기견을 입양해볼까 고민이 많아.', '맞아. 유기견 봉사활동 같은 것도 많은 것 같아. 나중에 기회가 되면 한번 가봐야겠어.', '요즘 밴드나 알아볼 수 있는 방법들이 많다고 들었어.  찾아보면 될 것 같아.', '맞아, 혼자 가서 할 수 있는 일도 아니야. 한번 알아봐.', '아니, 나도 관심은 있는데, 키우기는 조금 부담이 돼.']</끝>\\n### 사용자 페르소나: https://www.youtube.com/channel/UCZf0zflAxflAxflAxflAxw?view_as=subscriber#############################################\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(chat=chat) # epoch 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"사용자 페르소나: ['나는 반려동물을 키워보고 싶다.', '나는 유기견을 입양해보고 싶다.']</끝></끝><|endoftext|>\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(chat=chat) # epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### 사용자 채팅: ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이 많더라.', '대단하네. 나도 가끔 외로워서 반려동물을 키워보고 싶은데, 유기견을 입양해볼까 고민이 많아.', '맞아. 유기견 봉사활동 같은 것도 많은 것 같아. 나중에 기회가 되면 한번 가봐야겠어.', '요즘 밴드나 알아볼 수 있는 방법들이 많다고 들었어.  찾아보면 될 것 같아.', '맞아, 혼자 가서 할 수 있는 일도 아니야. 한번 알아봐.', '아니, 나도 관심은 있는데, 키우기는 조금 부담이 돼.']</끝>\\n### 사용자 페르소나: @'나는 반려동물을 키워보고 싶다.', @'나는 유기견을 입양해보고 싶다.', @'나는 혼자 가서 할 수 있는 일이 아니다.']</끝><|endoftext|>\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TrainOutput(global_step=292, training_loss=1.7602002000155514, metrics={'train_runtime': 319.9491, 'train_samples_per_second': 7.282, 'train_steps_per_second': 0.913, 'total_flos': 1.0667685003264e+16, 'train_loss': 1.7602002000155514\n",
    "gen(chat=chat) # epoch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### 사용자 채팅: ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이 많더라.', '대단하네. 나도 가끔 외로워서 반려동물을 키워보고 싶은데, 유기견을 입양해볼까 고민이 많아.', '맞아. 유기견 봉사활동 같은 것도 많은 것 같아. 나중에 기회가 되면 한번 가봐야겠어.', '요즘 밴드나 알아볼 수 있는 방법들이 많다고 들었어.  찾아보면 될 것 같아.', '맞아, 혼자 가서 할 수 있는 일도 아니야. 한번 알아봐.', '아니, 나도 관심은 있는데, 키우기는 조금 부담이 돼.']</끝>\\n### 사용자 페르소나: @이름/@@이다.></끝><|endoftext|>\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(chat=chat) # epoch 3"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png) - solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/donghae/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> ### 사용자 채팅: ['안녕, 잘 지냈어? 2일만이네.', '응, 혹시 반려동물을 키워? 요즘 유기견이 많더라.', '대단하네. 나도 가끔 외로워서 반려동물을 키워보고 싶은데, 유기견을 입양해볼까 고민이 많아.', '맞아. 유기견 봉사활동 같은 것도 많은 것 같아. 나중에 기회가 되면 한번 가봐야겠어.', '요즘 밴드나 알아볼 수 있는 방법들이 많다고 들었어.  찾아보면 될 것 같아.', '맞아, 혼자 가서 할 수 있는 일도 아니야. 한번 알아봐.', '아니, 나도 관심은 있는데, 키우기는 조금 부담이 돼.']</끝>\\n### 사용자 페르소나: 20대, 여성, 반려동물, 유기견, 봉사활동, 밴드</끝>\\n상기 표 1을 참조하면, 사용자 채팅은 사용자가 입력한 채팅 내용이고, 사용자 페르소나는 상기 사용자 채팅으로부터 추출된 키워드이다.\\n도 2는 본 발명의 일 실시예에 따른 사용자 페르소나를 추출하는 방법을 설명하기 위한 도면이다.\\n도 2를 참조하면\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(chat=chat) # solar 10.7B epoch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'EleutherAI/polyglot-ko-1.3b',\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    cache_dir='./cache'\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, './saved/polyglot-ko-1.3b-1ep')\n",
    "model = model.merge_and_unload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
